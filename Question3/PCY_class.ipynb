{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"z_yIooUgeCNC"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kE9YimL9Vlax","executionInfo":{"status":"ok","timestamp":1744002518591,"user_tz":-420,"elapsed":5759,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}},"outputId":"003495b7-ad63-4231-b864-188b5bbdc68d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fl3M2UGkaDHL","executionInfo":{"status":"ok","timestamp":1744002538413,"user_tz":-420,"elapsed":79,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}},"outputId":"2d9f3b43-e61a-4d31-a569-adcfe6bc8c53"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null"],"metadata":{"id":"ddV3DNq_Wdv2","executionInfo":{"status":"ok","timestamp":1744002544329,"user_tz":-420,"elapsed":3161,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}}},"execution_count":3,"outputs":[]},{"source":["!wget -q https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz"],"cell_type":"code","metadata":{"id":"YIRY1re-ZINz","executionInfo":{"status":"ok","timestamp":1744002609801,"user_tz":-420,"elapsed":14889,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"WR0FtpcaZF6i","executionInfo":{"status":"ok","timestamp":1744002622794,"user_tz":-420,"elapsed":5629,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}}},"outputs":[],"source":["# !cp /content/spark-3.1.1-bin-hadoop3.2.tgz .\n","!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n","!pip install -q findspark"]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""],"metadata":{"id":"uX59ksSEabyc","executionInfo":{"status":"ok","timestamp":1744002630541,"user_tz":-420,"elapsed":5,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import findspark\n","findspark.init()"],"metadata":{"id":"-_i2hXrRadda","executionInfo":{"status":"ok","timestamp":1744002633480,"user_tz":-420,"elapsed":2,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import pyspark as spark\n","from pyspark.sql import SparkSession, Row\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import StructType, StructField, \\\n","                ArrayType, StringType, IntegerType, FloatType\n","from itertools import combinations\n","\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","sc = spark.sparkContext"],"metadata":{"id":"MXM16TRYag2C","executionInfo":{"status":"ok","timestamp":1744002645791,"user_tz":-420,"elapsed":6588,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Load Dataset"],"metadata":{"id":"qE2R0eB_cB68"}},{"cell_type":"code","source":["\"\"\"Adjust as need\"\"\"\n","path = \"/content/drive/MyDrive/MMDS/Midterm/baskets.csv\"\n","\n","df = spark.read.csv(path, header=True)"],"metadata":{"id":"OCupll84aj-K","executionInfo":{"status":"ok","timestamp":1744003345862,"user_tz":-420,"elapsed":250,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["df = df.groupBy([\"Member_number\", \"Date\"]) \\\n","        .agg(F.sort_array(F.collect_set(\"itemDescription\")).alias(\"Basket\")) \\\n","        .select(\"Basket\")"],"metadata":{"id":"Xi-h908DcWHD","executionInfo":{"status":"ok","timestamp":1744003347408,"user_tz":-420,"elapsed":82,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["Data for PCY should be formated like this"],"metadata":{"id":"3BTFmORVfPSh"}},{"cell_type":"code","source":["df.show(5, truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cC5P89Eze2Na","executionInfo":{"status":"ok","timestamp":1744003386226,"user_tz":-420,"elapsed":1020,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}},"outputId":"b45f068f-1f85-401d-9d03-b8191bd4ccd3"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------------------------------------------------------+\n","|Basket                                                    |\n","+----------------------------------------------------------+\n","|[butter, fruit/vegetable juice, pork, sausage, whole milk]|\n","|[butter, sweet spreads]                                   |\n","|[oil, shopping bags]                                      |\n","|[frankfurter, white bread]                                |\n","|[curd, spices]                                            |\n","+----------------------------------------------------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["tmp = df.rdd"],"metadata":{"id":"nb6aKu6suBCP","executionInfo":{"status":"ok","timestamp":1744003676925,"user_tz":-420,"elapsed":18,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["PCY Class"],"metadata":{"id":"G3gPCxppmlcL"}},{"cell_type":"code","source":["class PCY:\n","\n","    def __init__(self, min_support=0.2, min_confidence=0.5, num_buckets=100):\n","        \"\"\"\n","        Params and Attributes:\n","            min_support: float, mimimum support for frequent itemsets\n","            min_confidence: float, minimum confidence for association rules\n","            num_buckets: int, number of buckets for PCY hash function\n","\n","            spark: SparkSession, SparkSession object\n","            df: DataFrame, DataFrame of baskets\n","            fq_pair_df: DataFrame, DataFrame of frequent pairs\n","            association_rules_df: DataFrame, DataFrame of association rules\n","            fq_item_count_dict: dict, dictionary of frequent items\n","                                                        and their counts\n","            fq_bucket_count_dict: dict, dictionary of frequent buckets\n","                                                        and their counts\n","        \"\"\"\n","        self.min_support = min_support\n","        self.min_confidence = min_confidence\n","        self.num_buckets = num_buckets\n","\n","        self.spark = SparkSession.builder.getOrCreate()\n","        self.df = None\n","        self.fq_pair_df = None\n","        self.association_rules_df = None\n","        self.min_count = 1\n","        self.fq_item_count_dict = dict()\n","        self.fq_bucket_count_dict = dict()\n","\n","    def fit(self, df):\n","        \"\"\"\n","        Fit the model to the data.\n","        Finding frequent items in Pass 1\n","        Finding frequent pairs in Pass 2\n","        Finding association rules\n","\n","        Params:\n","            df: DataFrame, DataFrame of baskets\n","        \"\"\"\n","\n","        self.df = df\n","\n","        \"\"\"Miminum count for frequent itemsets\"\"\"\n","        self.min_count = int(self.min_support * self.df.count())\n","\n","        self.pass1()\n","        self.pass2()\n","        self.association_rules()\n","\n","\n","    def pass1(self):\n","        \"\"\"\n","        Finding frequent items and frequent buckets in Pass 1\n","        \"\"\"\n","        self.item_count()\n","        self.bucket_count()\n","\n","    def item_count(self):\n","\n","        \"\"\"\n","        Item Dataframe contains item name and its count.\n","        This step is as know as finding candidate items, C1\n","        \"\"\"\n","        item_df = self.df.select(F.explode(self.df.columns[0]).alias(\"Item\"))\n","\n","        \"\"\"Finding frequent items, L1\"\"\"\n","        item_count_df = (item_df.groupBy(\"Item\").count()\n","                    .filter(F.col(\"count\") >= self.min_count))\n","\n","        \"\"\"\n","        Handle cases where the frequent item DataFrame is empty,\n","        indicating that no items meet the minimum frequency threshold.\n","        \"\"\"\n","        if item_count_df.rdd.isEmpty():\n","            raise ValueError(\"No frequent items found with the \" +\n","                                \"given minimum support.\")\n","        self.fq_item_count_dict = (item_count_df.rdd.collectAsMap())\n","\n","\n","    @staticmethod\n","    def hash_pair(item1: str, item2: str, num_buckets: int) -> int:\n","        \"\"\"Hash function for pairs\"\"\"\n","\n","        def hash_item(item):\n","            \"\"\"\n","            Hash function for an item.\n","\n","            Each character in item string will convert to ASCII value,\n","            then multiples it by 31 ^ its position in the string.\n","\n","            This function ensures hash values would intend to distribute\n","            into buckets uniformly as possible and amplifies the position of\n","            each character.\n","            \"\"\"\n","            return sum(ord(ch) * 31**i for i, ch in enumerate(item))\n","\n","        return (hash_item(item1) ^ hash_item(item2)) % num_buckets\n","\n","\n","    def bucket_count(self):\n","        \"\"\"\n","        Finding frequent buckets in Pass 1\n","\n","        Because we have already found frequent items by PySpark DataFrame.\n","        We just consider pairs of those frequent items and hash these pairs\n","        to the buckets. This helps us use memory efficiently.\n","\n","        Note that original PCY hash all pairs of a basket to buckets\n","        without considering whether they are frequent or not.\n","        \"\"\"\n","\n","        \"\"\"\n","            Could not pass \"self\" into transformation and operation function.\n","            We need some variable to capture those attributes.\n","        \"\"\"\n","        fq_items = list(self.fq_item_count_dict.keys())\n","        num_buckets = self.num_buckets\n","        min_count = self.min_count\n","        hash_pair = PCY.hash_pair\n","\n","        def hash_pair_to_bucket(items):\n","            \"\"\"\n","            Create RDD, its key=bucket, value=1.\n","\n","            This is for counting the number of pairs in\n","            each bucket for transformation function in later.\n","            \"\"\"\n","            buckets = []\n","            n = len(items)\n","            for i in range(n - 1):\n","                for j in range(i + 1, n):\n","                    if items[i] in fq_items and items[j] in fq_items:\n","                        bucket = hash_pair(items[i], items[j], num_buckets)\n","                        buckets.append((bucket, 1))\n","            return buckets\n","\n","\n","        \"\"\"Finding frequent buckets, B1\"\"\"\n","        self.fq_bucket_count_dict = (self.df.rdd\n","                    .flatMap(lambda basket: hash_pair_to_bucket(basket[0]))\n","                    .reduceByKey(lambda x, y: x + y)\n","                    .filter(lambda x: x[1] >= min_count)\n","                    .collectAsMap())\n","\n","\n","    def pass2(self):\n","\n","        hash_pair = PCY.hash_pair\n","        num_buckets = self.num_buckets\n","        fq_items = list(self.fq_item_count_dict.keys())\n","        fq_buckets = list(self.fq_bucket_count_dict.keys())\n","        min_count = self.min_count\n","\n","        def find_fq_pairs(items):\n","            \"\"\"\n","            Finding frequent pairs of each basket\n","            \"\"\"\n","\n","            pairs = []\n","            n = len(items)\n","\n","            \"\"\"\n","            Iterate through all distinct pairs of items in the basket.\n","            Check if the hash value of each pair exists in the hash table.\n","            \"\"\"\n","            for i in range(n - 1):\n","                for j in range(i + 1, n):\n","                    if items[i] in fq_items and items[j] in fq_items:\n","                        bucket = hash_pair(items[i], items[j], num_buckets)\n","                        if bucket in fq_buckets:\n","                            pairs.append((items[i], items[j]))\n","            return pairs\n","\n","        fq_pair_count = (self.df.rdd\n","                    .flatMap(lambda basket: find_fq_pairs(basket[0]))\n","                    .map(lambda x: (tuple(sorted(x)), 1))\n","                    .reduceByKey(lambda x, y: x + y)\n","                    .filter(lambda x: x[1] >= min_count))\n","\n","        # xem lại chưa khai báo fq_pair_count_(dict)\n","        self.fq_pair_count = fq_pair_count.collectAsMap()\n","\n","        self.create_fq_pair_df(fq_pair_count)\n","\n","    def create_fq_pair_df(self, rdd):\n","        \"\"\"\n","        Create DataFrame of frequent pairs\n","\n","        Params:\n","            rdd: RDD, RDD of frequent pairs\n","        \"\"\"\n","\n","        schema = StructType([\n","            StructField(\"Pair\", ArrayType(StringType())),\n","            StructField(\"Support\", IntegerType())\n","        ])\n","\n","        self.fq_pair_df = (spark.createDataFrame(rdd\n","                            .map(lambda x: Row(Pair=list(x[0]), Support=x[1])),\n","                                                 schema))\n","\n","    def association_rules(self):\n","        \"\"\"\n","        Finding association rules from frequent pairs\n","        \"\"\"\n","\n","        fq_item_count_dict = self.fq_item_count_dict\n","        min_confidence = self.min_confidence\n","\n","        def generate_association_rules(pair, pair_support):\n","            \"\"\"\n","            Generate association rules from frequent pairs\n","            \"\"\"\n","\n","            rules = []\n","\n","            item1, item2 = pair\n","            item1_support = fq_item_count_dict[str(item1)]\n","            item2_support = fq_item_count_dict[str(item2)]\n","\n","            \"\"\"Case 1: item1 -> item2\"\"\"\n","            if item1_support > 0:\n","                confidence = pair_support / item1_support\n","                if confidence >= min_confidence:\n","                    rules.append((item1, item2, confidence))\n","\n","            \"\"\"Case 2: item2 -> item1\"\"\"\n","            if item2_support > 0:\n","                confidence = pair_support / item2_support\n","                if confidence >= min_confidence:\n","                    rules.append((item2, item1, confidence))\n","\n","            return rules\n","\n","        \"\"\"\n","        Create Association rules user-defined function in PySpark DataFrame\n","        \"\"\"\n","        generate_association_rules_udf = F.udf(\n","            lambda pair, pair_support:\n","            generate_association_rules(pair, pair_support),\n","\n","            ArrayType(\n","                StructType([\n","                    StructField(\"Antecedent\", StringType(), False),\n","                    StructField(\"Consequence\", StringType(), False),\n","                    StructField(\"Confidence\", FloatType(), False)\n","                ])\n","            )\n","        )\n","\n","        \"\"\"\n","        Create Association rules DataFrame\n","        \"\"\"\n","        self.association_rules_df = (self.fq_pair_df\n","                    .withColumn(\"Rules\",\n","                    generate_association_rules_udf(\"Pair\", \"Support\"))\n","                    .select(F.explode(\"Rules\").alias(\"Rule\"))\n","                    .select(\"Rule.*\")\n","                    .dropna())\n","\n"],"metadata":{"id":"c5kpeG09ce7C","executionInfo":{"status":"ok","timestamp":1744003522932,"user_tz":-420,"elapsed":12,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["# Test PCY"],"metadata":{"id":"Fm1uOBb2naoz"}},{"cell_type":"code","source":["data = df.select(F.col(\"Basket\").cast(ArrayType(StringType())))"],"metadata":{"id":"Q2VdNElte_NP","executionInfo":{"status":"ok","timestamp":1744003533978,"user_tz":-420,"elapsed":30,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["\n","pcy = PCY(min_support=0.005, min_confidence=0.05, num_buckets=13)\n","pcy.fit(data)"],"metadata":{"id":"aVSQ6y6a2sBY","colab":{"base_uri":"https://localhost:8080/","height":914},"executionInfo":{"status":"error","timestamp":1744003551246,"user_tz":-420,"elapsed":5458,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}},"outputId":"cdd49786-f109-45ab-9c49-d98702a45de2"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 437, in dumps\n","    return cloudpickle.dumps(obj, pickle_protocol)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 72, in dumps\n","    cp.dump(obj)\n","  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 540, in dump\n","    return Pickler.dump(self, obj)\n","           ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 630, in reducer_override\n","    return self._function_reduce(obj)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 503, in _function_reduce\n","    return self._dynamic_function_reduce(obj)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 484, in _dynamic_function_reduce\n","    state = _function_getstate(func)\n","            ^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 156, in _function_getstate\n","    f_globals_ref = _extract_code_globals(func.__code__)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in _extract_code_globals\n","    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n","                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in <setcomp>\n","    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n","                 ~~~~~^^^^^^^\n","IndexError: tuple index out of range\n"]},{"output_type":"error","ename":"PicklingError","evalue":"Could not serialize object: IndexError: tuple index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mreducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamic_function_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mnewargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_getnewargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_function_getstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         return (types.FunctionType, newargs, state, None, None,\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mf_globals_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_code_globals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     f_globals = {k: func.__globals__[k] for k in f_globals_ref if k in\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mout_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moparg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moparg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_walk_global_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mout_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moparg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moparg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_walk_global_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: tuple index out of range","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-b3660fd3f6cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpcy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_support\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_confidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_buckets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpcy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-35-5237f7fffb1f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_support\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massociation_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-5237f7fffb1f>\u001b[0m in \u001b[0;36mpass1\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mFinding\u001b[0m \u001b[0mfrequent\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfrequent\u001b[0m \u001b[0mbuckets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPass\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \"\"\"\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-5237f7fffb1f>\u001b[0m in \u001b[0;36mitem_count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mindicating\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mno\u001b[0m \u001b[0mitems\u001b[0m \u001b[0mmeet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mminimum\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \"\"\"\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mitem_count_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEmpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             raise ValueError(\"No frequent items found with the \" +\n\u001b[1;32m     77\u001b[0m                                 \"given minimum support.\")\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36misEmpty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m         \"\"\"\n\u001b[0;32m-> 1606\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msaveAsNewAPIHadoopDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2947\u001b[0m             \u001b[0mprofiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2949\u001b[0;31m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0m\u001b[1;32m   2950\u001b[0m                                       self._jrdd_deserializer, profiler)\n\u001b[1;32m   2951\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2830\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2812\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2814\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2815\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBroadcastThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"]}]},{"cell_type":"markdown","source":["# Association rules that generated by PCY"],"metadata":{"id":"T6PZZpv5oGHL"}},{"cell_type":"code","source":["pcy.association_rules_df.show(5, truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2WAloOHtSzBa","executionInfo":{"status":"ok","timestamp":1730129159170,"user_tz":-420,"elapsed":9917,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}},"outputId":"ee246f82-273d-464c-c5e7-996787bb1cfd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----------+----------+\n","|Antecedent |Consequence|Confidence|\n","+-----------+-----------+----------+\n","|rolls/buns |whole milk |0.12697448|\n","|whole milk |rolls/buns |0.08844689|\n","|canned beer|whole milk |0.12820514|\n","|frankfurter|whole milk |0.139823  |\n","|sausage    |yogurt     |0.0952381 |\n","+-----------+-----------+----------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"markdown","source":["# Frequent pairs that generated by PCY"],"metadata":{"id":"PIpZJCiHoOrL"}},{"cell_type":"code","source":["pcy.fq_pair_df.show(5, truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4g_Kqqq8ePkF","executionInfo":{"status":"ok","timestamp":1730129246158,"user_tz":-420,"elapsed":8499,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}},"outputId":"bcb7b927-3e4d-4e43-d9ab-9b61f0ba5cf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------------+-------+\n","|Pair                     |Support|\n","+-------------------------+-------+\n","|[rolls/buns, whole milk] |209    |\n","|[canned beer, whole milk]|90     |\n","|[frankfurter, whole milk]|79     |\n","|[sausage, yogurt]        |86     |\n","|[pip fruit, rolls/buns]  |74     |\n","+-------------------------+-------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"markdown","source":["# Validate by FPGrowth"],"metadata":{"id":"uYP0Vzu2nhfT"}},{"cell_type":"code","source":["from pyspark.ml.fpm import FPGrowth\n","\n","fpGrowth = FPGrowth(itemsCol=\"Basket\", minSupport=0.2, minConfidence=0.15)\n","model = fpGrowth.fit(data)"],"metadata":{"id":"9lwCiPkMiRTN","executionInfo":{"status":"ok","timestamp":1743999872944,"user_tz":-420,"elapsed":8776,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["model.freqItemsets.filter(F.size(\"items\") >= 2).show(5, truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k7VgOyikifB-","executionInfo":{"status":"ok","timestamp":1730129255904,"user_tz":-420,"elapsed":1026,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}},"outputId":"5e4372b7-cd3c-439e-a766-0647d17c6f9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------+----+\n","|items                         |freq|\n","+------------------------------+----+\n","|[other vegetables, whole milk]|222 |\n","|[rolls/buns, other vegetables]|158 |\n","|[rolls/buns, whole milk]      |209 |\n","|[soda, rolls/buns]            |121 |\n","|[soda, other vegetables]      |145 |\n","+------------------------------+----+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["model.associationRules.show(5, truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NFm1zCvcjErt","executionInfo":{"status":"ok","timestamp":1743999886307,"user_tz":-420,"elapsed":4790,"user":{"displayName":"Phát Trần Văn","userId":"11071567766159891267"}},"outputId":"abe765d6-fd82-42d0-f3f7-b567f44ffe6f"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+----------+----------+----+-------+\n","|antecedent|consequent|confidence|lift|support|\n","+----------+----------+----------+----+-------+\n","+----------+----------+----------+----+-------+\n","\n"]}]}]}